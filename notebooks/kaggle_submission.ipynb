{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ECG Image Digitization - Kaggle Submission Notebook\n\nThis notebook trains an ECG digitization model and generates a submission file for the PhysioNet Challenge.\n\n**Competition**: [PhysioNet ECG Image Digitization](https://www.kaggle.com/competitions/physionet-ecg-image-digitization)\n\n## Pipeline Overview\n1. ‚úÖ Environment Setup\n2. ‚úÖ Dataset Loading\n3. ‚úÖ Model Training\n4. ‚úÖ Inference\n5. ‚úÖ Submission Generation","metadata":{}},{"cell_type":"markdown","source":"## 1. Environment Setup\n\nInstall dependencies and detect Kaggle environment.","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\nfrom pathlib import Path\n\n# Detect Kaggle environment\nIS_KAGGLE = 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\nprint(f\"Running on Kaggle: {IS_KAGGLE}\")\n\n# Install additional dependencies if needed\nif IS_KAGGLE:\n    # !pip install -q \"numpy<2\" \"pandas>=2.2.2\" segmentation-models-pytorch hydra-core omegaconf wfdb neurokit2 biosppy loguru richsegmentation-models-pytorch hydra-core omegaconf wfdb neurokit2 biosppy loguru rich\n    \n    # Set paths for Kaggle\n    DATA_DIR = Path('/kaggle/input/physionet-ecg-image-digitization')\n    OUTPUT_DIR = Path('/kaggle/working')\nelse:\n    # Local paths\n    DATA_DIR = Path('../data')\n    OUTPUT_DIR = Path('../models')\n\nprint(f\"Data directory: {DATA_DIR}\")\nprint(f\"Output directory: {OUTPUT_DIR}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T19:15:07.624205Z","iopub.execute_input":"2025-12-26T19:15:07.624970Z","iopub.status.idle":"2025-12-26T19:15:09.894353Z","shell.execute_reply.started":"2025-12-26T19:15:07.624941Z","shell.execute_reply":"2025-12-26T19:15:09.893599Z"}},"outputs":[{"name":"stdout","text":"Running on Kaggle: True\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: Ignored the following versions that require a different python version: 1.21.2 Requires-Python >=3.7,<3.11; 1.21.3 Requires-Python >=3.7,<3.11; 1.21.4 Requires-Python >=3.7,<3.11; 1.21.5 Requires-Python >=3.7,<3.11; 1.21.6 Requires-Python >=3.7,<3.11\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement richsegmentation-models-pytorch (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for richsegmentation-models-pytorch\u001b[0m\u001b[31m\n\u001b[0mData directory: /kaggle/input/physionet-ecg-image-digitization\nOutput directory: /kaggle/working\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## 2. Dataset Loading\n\nLoad and verify the competition dataset.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n# List available files (limit to first 10 for readability)\nif DATA_DIR.exists():\n    print(\"\\nüìÅ Sample files:\")\n    for idx, item in enumerate(sorted(DATA_DIR.rglob('*'))):\n        if item.is_file() and idx < 10:\n            print(f\"  {item.relative_to(DATA_DIR)}\")\n        elif idx >= 10:\n            print(\"  ... (more files)\")\n            break\nelse:\n    print(f\"‚ö†Ô∏è  Data directory not found: {DATA_DIR}\")\n    print(\"Please ensure the competition data is linked/downloaded.\")\n\n# Load metadata if available\ntrain_csv = DATA_DIR / 'train.csv'\nif train_csv.exists():\n    train_df = pd.read_csv(train_csv)\n    print(f\"\\nüìä Training samples: {len(train_df)}\")\n    print(f\"Columns: {list(train_df.columns)}\")\n    display(train_df.head())\nelse:\n    print(f\"‚ö†Ô∏è  Training metadata not found: {train_csv}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T19:09:06.275442Z","iopub.execute_input":"2025-12-26T19:09:06.275697Z","iopub.status.idle":"2025-12-26T19:09:15.568785Z","shell.execute_reply.started":"2025-12-26T19:09:06.275672Z","shell.execute_reply":"2025-12-26T19:09:15.567666Z"}},"outputs":[{"name":"stdout","text":"\nüìÅ Sample files:\n  sample_submission.parquet\n  test/1053922973.png\n  test/2352854581.png\n  test.csv\n  train/1006427285/1006427285-0001.png\n  train/1006427285/1006427285-0003.png\n  train/1006427285/1006427285-0004.png\n  ... (more files)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/523170441.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mtrain_csv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDATA_DIR\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtrain_csv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_csv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nüìä Training samples: {len(train_df)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Columns: {list(train_df.columns)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1966\u001b[0m                 \u001b[0mnew_col_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcol_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1968\u001b[0;31m             df = DataFrame(\n\u001b[0m\u001b[1;32m   1969\u001b[0m                 \u001b[0mnew_col_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1970\u001b[0m                 \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseries\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m         \u001b[0mmissing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mensure_index\u001b[0;34m(index_like, copy)\u001b[0m\n\u001b[1;32m   7645\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mMultiIndex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_like\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7646\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7647\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtupleize_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7648\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7649\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, data, dtype, copy, name, tupleize_cols)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"index must be specified when data is not list-like\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/construction.py\u001b[0m in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_convert_platform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    655\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msubarr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m                 \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36mmaybe_convert_platform\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_dtype_obj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_convert_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.maybe_convert_objects\u001b[0;34m()\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Cannot convert numpy.ndarray to numpy.ndarray"],"ename":"TypeError","evalue":"Cannot convert numpy.ndarray to numpy.ndarray","output_type":"error"}],"execution_count":2},{"cell_type":"markdown","source":"### Visualize Sample ECG Images","metadata":{}},{"cell_type":"code","source":"# Visualize a few sample images\n# Images are in nested directories (train/ecg001/, train/ecg002/, etc.)\ntrain_images = DATA_DIR / 'train'\nif train_images.exists():\n    # Use rglob to recursively find images in subdirectories\n    image_files = list(train_images.rglob('*.png'))[:6]\n    \n    if len(image_files) > 0:\n        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n        axes = axes.flatten()\n        \n        for idx, img_path in enumerate(image_files):\n            img = Image.open(img_path)\n            axes[idx].imshow(img, cmap='gray')\n            # Show relative path for context\n            axes[idx].set_title(str(img_path.relative_to(train_images)), fontsize=8)\n            axes[idx].axis('off')\n        \n        plt.tight_layout()\n        plt.show()\n        print(f\"\\n‚úÖ Displayed {len(image_files)} sample images\")\n    else:\n        print(f\"‚ö†Ô∏è  No PNG images found in {train_images}\")\nelse:\n    print(f\"‚ö†Ô∏è  Training images directory not found: {train_images}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T19:09:15.569345Z","iopub.status.idle":"2025-12-26T19:09:15.569654Z","shell.execute_reply.started":"2025-12-26T19:09:15.569479Z","shell.execute_reply":"2025-12-26T19:09:15.569497Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Configure Training Pipeline\n\nSet up Hydra configuration programmatically with MLflow **disabled** for Kaggle.","metadata":{}},{"cell_type":"code","source":"from omegaconf import OmegaConf, DictConfig\nimport torch\n\n# Create configuration\ncfg = OmegaConf.create({\n    'project': {\n        'name': 'ecg-digitization',\n        'version': '0.1.0',\n        'seed': 42,\n    },\n    'mlflow': {\n        'enabled': False,  # DISABLED for Kaggle\n        'tracking_uri': 'http://localhost:5050',\n        'experiment_name': 'ecg-digitization-kaggle',\n    },\n    'paths': {\n        'data_dir': str(DATA_DIR),\n        'train_dir': str(DATA_DIR / 'train'),\n        'test_dir': str(DATA_DIR / 'test'),\n        'output_dir': str(OUTPUT_DIR / 'models'),\n        'checkpoint_dir': str(OUTPUT_DIR / 'checkpoints'),\n        'submission_dir': str(OUTPUT_DIR),\n        'log_dir': str(OUTPUT_DIR / 'logs'),\n    },\n    'data': {\n        'image_size': [512, 512],\n        'batch_size': 4 if IS_KAGGLE else 8,  # Smaller batch for Kaggle GPU\n        'num_workers': 2,\n        'pin_memory': True,\n        'augment_prob': 0.5,\n    },\n    'model': {\n        'encoder_name': 'resnet50',\n        'encoder_weights': 'imagenet',\n        'num_leads': 12,\n        'signal_length': 5000,\n    },\n    'training': {\n        'epochs': 10 if IS_KAGGLE else 20,  # Fewer epochs for Kaggle time limits\n        'learning_rate': 1e-4,\n        'weight_decay': 1e-5,\n        'val_split': 0.2,\n    },\n    'approach': {\n        'method': 'baseline',\n    },\n})\n\n# Set random seed\ntorch.manual_seed(cfg.project.seed)\nnp.random.seed(cfg.project.seed)\n\n# Create output directories\nfor dir_path in [cfg.paths.output_dir, cfg.paths.checkpoint_dir, cfg.paths.log_dir]:\n    Path(dir_path).mkdir(parents=True, exist_ok=True)\n\nprint(\"\\n‚öôÔ∏è  Configuration:\")\nprint(OmegaConf.to_yaml(cfg))\nprint(f\"\\nüîß MLflow tracking: {'‚úÖ ENABLED' if cfg.mlflow.enabled else '‚ùå DISABLED'}\")\nprint(f\"üñ•Ô∏è  Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T19:09:15.570993Z","iopub.status.idle":"2025-12-26T19:09:15.571487Z","shell.execute_reply.started":"2025-12-26T19:09:15.571302Z","shell.execute_reply":"2025-12-26T19:09:15.571317Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Model Training\n\nTrain the ECG digitization model using our pipeline.","metadata":{}},{"cell_type":"code","source":"# Add src to path if running locally\nif not IS_KAGGLE:\n    src_path = Path('../src')\n    if src_path.exists() and str(src_path) not in sys.path:\n        sys.path.insert(0, str(src_path.resolve()))\n\nfrom torch.utils.data import DataLoader, random_split\nfrom ecg_digitization.data import ECGImageDataset, get_train_transforms, get_val_transforms, collate_fn\nfrom ecg_digitization.models import ECGDigitizer\nfrom ecg_digitization.training import ECGTrainer, CombinedLoss\nfrom ecg_digitization.utils.mlflow_utils import create_mlflow_tracker\nfrom ecg_digitization.utils import setup_logging\n\n# Setup logging\nsetup_logging(cfg.paths.log_dir)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"\\nüéØ Training on: {device}\")\n\n# Initialize MLflow tracker (will be no-op since mlflow.enabled=False)\nmlflow_tracker = create_mlflow_tracker(\n    enabled=cfg.mlflow.enabled,\n    tracking_uri=cfg.mlflow.tracking_uri,\n    experiment_name=cfg.mlflow.experiment_name,\n    run_name=\"kaggle_training\",\n    tags={\"environment\": \"kaggle\" if IS_KAGGLE else \"local\"},\n)\n\n# Start MLflow run (no-op if disabled)\nmlflow_tracker.start_run()\n\ntry:\n    # Log config (no-op if disabled)\n    config_dict = OmegaConf.to_container(cfg, resolve=True)\n    mlflow_tracker.log_config(config_dict)\n    \n    # Create datasets\n    print(\"\\nüì¶ Preparing datasets...\")\n    train_transform = get_train_transforms(tuple(cfg.data.image_size), cfg.data.augment_prob)\n    val_transform = get_val_transforms(tuple(cfg.data.image_size))\n    \n    full_dataset = ECGImageDataset(\n        cfg.paths.data_dir,\n        transform=train_transform,\n        is_train=True,\n    )\n    \n    # Split into train/val\n    val_size = int(len(full_dataset) * cfg.training.val_split)\n    train_size = len(full_dataset) - val_size\n    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n    val_dataset.dataset.transform = val_transform\n    \n    print(f\"  Training samples: {train_size}\")\n    print(f\"  Validation samples: {val_size}\")\n    \n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=cfg.data.batch_size,\n        shuffle=True,\n        num_workers=cfg.data.num_workers,\n        pin_memory=cfg.data.pin_memory,\n        collate_fn=collate_fn,\n    )\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=cfg.data.batch_size,\n        shuffle=False,\n        num_workers=cfg.data.num_workers,\n        pin_memory=cfg.data.pin_memory,\n        collate_fn=collate_fn,\n    )\n    \n    # Create model\n    print(\"\\nüèóÔ∏è  Building model...\")\n    model = ECGDigitizer(\n        encoder_name=cfg.model.encoder_name,\n        encoder_weights=cfg.model.encoder_weights,\n        num_leads=cfg.model.num_leads,\n        signal_length=cfg.model.signal_length,\n    )\n    \n    # Setup training\n    optimizer = torch.optim.AdamW(\n        model.parameters(),\n        lr=cfg.training.learning_rate,\n        weight_decay=cfg.training.weight_decay,\n    )\n    \n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n        optimizer, T_max=cfg.training.epochs\n    )\n    \n    criterion = CombinedLoss()\n    \n    # Create trainer with MLflow integration (will be no-op)\n    trainer = ECGTrainer(\n        model=model,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        optimizer=optimizer,\n        criterion=criterion,\n        scheduler=scheduler,\n        device=device,\n        checkpoint_dir=cfg.paths.checkpoint_dir,\n        mlflow_tracker=mlflow_tracker,\n    )\n    \n    # Train model\n    print(f\"\\nüöÄ Starting training for {cfg.training.epochs} epochs...\")\n    print(\"=\" * 60)\n    trainer.train(cfg.training.epochs)\n    print(\"=\" * 60)\n    print(\"\\n‚úÖ Training completed!\")\n    \n    # Plot training curves\n    fig, ax = plt.subplots(figsize=(10, 6))\n    epochs = range(1, len(trainer.train_losses) + 1)\n    ax.plot(epochs, trainer.train_losses, 'b-', label='Training Loss', linewidth=2)\n    ax.plot(epochs, trainer.val_losses, 'r-', label='Validation Loss', linewidth=2)\n    ax.set_xlabel('Epoch', fontsize=12)\n    ax.set_ylabel('Loss', fontsize=12)\n    ax.set_title('Training Progress', fontsize=14, fontweight='bold')\n    ax.legend(fontsize=11)\n    ax.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n    \n    print(f\"\\nüìà Best validation loss: {trainer.best_val_loss:.4f}\")\n    \n    # End MLflow run (no-op if disabled)\n    mlflow_tracker.end_run(status=\"FINISHED\")\n    \nexcept Exception as e:\n    print(f\"\\n‚ùå Training failed: {e}\")\n    mlflow_tracker.end_run(status=\"FAILED\")\n    raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T19:09:15.571887Z","iopub.status.idle":"2025-12-26T19:09:15.572125Z","shell.execute_reply.started":"2025-12-26T19:09:15.572015Z","shell.execute_reply":"2025-12-26T19:09:15.572025Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Inference & Submission Generation\n\nGenerate predictions on the test set and create submission file.","metadata":{}},{"cell_type":"code","source":"from ecg_digitization.inference import ECGPredictor\n\nprint(\"\\nüîÆ Running inference on test set...\")\n\n# Prepare test dataset\ntest_transform = get_val_transforms(tuple(cfg.data.image_size))\ntest_dataset = ECGImageDataset(\n    cfg.paths.data_dir,\n    transform=test_transform,\n    is_train=False,\n)\n\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=cfg.data.batch_size,\n    shuffle=False,\n    num_workers=cfg.data.num_workers,\n    collate_fn=collate_fn,\n)\n\nprint(f\"  Test samples: {len(test_dataset)}\")\n\n# Load best model\nmodel = ECGDigitizer(\n    encoder_name=cfg.model.encoder_name,\n    num_leads=cfg.model.num_leads,\n    signal_length=cfg.model.signal_length,\n)\n\npredictor = ECGPredictor(\n    model=model,\n    checkpoint_path=f\"{cfg.paths.checkpoint_dir}/best_model.pt\",\n    device=device,\n)\n\n# Generate predictions\npredictions = predictor.predict(test_loader)\nprint(f\"\\n‚úÖ Generated predictions for {len(predictions)} samples\")\n\n# Load test metadata\ntest_csv = Path(cfg.paths.data_dir) / 'test.csv'\nif test_csv.exists():\n    metadata = pd.read_csv(test_csv)\n    print(f\"  Loaded metadata for {len(metadata)} test samples\")\nelse:\n    metadata = None\n    print(\"  ‚ö†Ô∏è  No test metadata found\")\n\n# Generate submission file\nsubmission_path = Path(cfg.paths.submission_dir) / 'submission.parquet'\npredictor.generate_submission(\n    predictions,\n    str(submission_path),\n    metadata,\n)\n\nprint(f\"\\nüìù Submission file created: {submission_path}\")\nprint(f\"  File size: {submission_path.stat().st_size / 1024 / 1024:.2f} MB\")\n\n# Verify submission format\nif submission_path.exists():\n    submission_df = pd.read_parquet(submission_path)\n    print(f\"\\n‚úÖ Submission verification:\")\n    print(f\"  Shape: {submission_df.shape}\")\n    print(f\"  Columns: {list(submission_df.columns)}\")\n    display(submission_df.head())\nelse:\n    print(\"\\n‚ùå Submission file not created!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T19:09:15.573166Z","iopub.status.idle":"2025-12-26T19:09:15.573464Z","shell.execute_reply.started":"2025-12-26T19:09:15.573286Z","shell.execute_reply":"2025-12-26T19:09:15.573300Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Visualize Sample Predictions\n\nDisplay some sample predictions to verify quality.","metadata":{}},{"cell_type":"code","source":"# Visualize a few predictions\nnum_samples = min(3, len(predictions))\n\nfig, axes = plt.subplots(num_samples, 1, figsize=(14, 4 * num_samples))\nif num_samples == 1:\n    axes = [axes]\n\nfor idx in range(num_samples):\n    pred = predictions[idx]\n    time = np.arange(pred.shape[1]) / 500  # Assuming 500 Hz sampling rate\n    \n    # Plot all 12 leads\n    for lead_idx in range(min(12, pred.shape[0])):\n        axes[idx].plot(time, pred[lead_idx, :] + lead_idx * 2, linewidth=0.8, alpha=0.8)\n    \n    axes[idx].set_xlabel('Time (s)', fontsize=11)\n    axes[idx].set_ylabel('Lead (offset)', fontsize=11)\n    axes[idx].set_title(f'Sample {idx + 1} - Predicted ECG Signals', fontsize=12, fontweight='bold')\n    axes[idx].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\n‚úÖ Displayed {num_samples} sample predictions\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T19:09:15.574286Z","iopub.status.idle":"2025-12-26T19:09:15.574559Z","shell.execute_reply.started":"2025-12-26T19:09:15.574443Z","shell.execute_reply":"2025-12-26T19:09:15.574457Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üéâ Submission Ready!\n\nYour submission file has been generated and is ready to submit to Kaggle.\n\n**Next Steps**:\n1. Download `submission.parquet` from the output directory\n2. Submit to the [PhysioNet ECG Image Digitization competition](https://www.kaggle.com/competitions/physionet-ecg-image-digitization)\n3. Check your leaderboard score!\n\n**Note**: MLflow tracking was disabled for this Kaggle run. To enable tracking locally with MLflow, set `mlflow.enabled=true` in the configuration.","metadata":{}}]}